{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pytesseract\n",
    "from imutils.object_detection import non_max_suppression\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Tesseract executable path manually\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL_PATH = 'models\\\\frozen_east_text_detection.pb'\n",
    "SAVED_MODEL_PATH = 'https://tfhub.dev/captain-pool/esrgan-tf2/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hub.load(SAVED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample1.jpg', 'test1.png']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = '..\\\\images\\\\test_images\\\\'\n",
    "os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dict = {}\n",
    "tesseract_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(scores, geometry, confThreshold=0.5):\n",
    "    \"\"\"\n",
    "    Decodes the predictions from the EAST text detector model.\n",
    "\n",
    "    Args:\n",
    "        scores (numpy.ndarray): The scores from the EAST detector.\n",
    "        geometry (numpy.ndarray): The geometry from the EAST detector.\n",
    "        confThreshold (float): Confidence threshold to filter weak detections.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the bounding boxes and associated confidences.\n",
    "    \"\"\"\n",
    "    # Grab the number of rows and columns from the scores volume, then\n",
    "    # initialize our set of bounding box rectangles and corresponding\n",
    "    # confidence scores\n",
    "    (numRows, numCols) = scores.shape[2:4]\n",
    "    rects = []\n",
    "    confidences = []\n",
    "\n",
    "    # Loop over the number of rows\n",
    "    for y in range(0, numRows):\n",
    "        # Extract the scores (probabilities), followed by the\n",
    "        # geometrical data used to derive potential bounding box\n",
    "        # coordinates that surround text\n",
    "        scoresData = scores[0, 0, y]\n",
    "        xData0 = geometry[0, 0, y]\n",
    "        xData1 = geometry[0, 1, y]\n",
    "        xData2 = geometry[0, 2, y]\n",
    "        xData3 = geometry[0, 3, y]\n",
    "        anglesData = geometry[0, 4, y]\n",
    "\n",
    "        # Loop over the number of columns\n",
    "        for x in range(0, numCols):\n",
    "            # If our score does not have sufficient probability,\n",
    "            # ignore it\n",
    "            if scoresData[x] < confThreshold:\n",
    "                continue\n",
    "\n",
    "            # Compute the offset factor as our resulting feature\n",
    "            # maps will be 4x smaller than the input image\n",
    "            (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
    "\n",
    "            # Extract the rotation angle for the prediction and\n",
    "            # compute the sin and cosine\n",
    "            angle = anglesData[x]\n",
    "            cos = np.cos(angle)\n",
    "            sin = np.sin(angle)\n",
    "\n",
    "            # Use the geometry volume to derive the width and height\n",
    "            # of the bounding box\n",
    "            h = xData0[x] + xData2[x]\n",
    "            w = xData1[x] + xData3[x]\n",
    "\n",
    "            # Compute both the starting and ending (x, y)-coordinates\n",
    "            # for the text prediction bounding box\n",
    "            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
    "            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
    "            startX = int(endX - w)\n",
    "            startY = int(endY - h)\n",
    "\n",
    "            # Add the bounding box coordinates and probability score\n",
    "            # to our respective lists\n",
    "            rects.append((startX, startY, endX, endY))\n",
    "            confidences.append(scoresData[x])\n",
    "\n",
    "    # Return a tuple of the bounding boxes and associated confidences\n",
    "    return (rects, confidences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(orig, scores, geometry, confThreshold=0.5, nmsThreshold=0.4, rW=1, rH=1):\n",
    "    \"\"\"\n",
    "    Post-processes the output of the EAST text detector model to suppress weak, overlapping bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        orig (numpy.ndarray): The original image.\n",
    "        scores (numpy.ndarray): The scores from the EAST detector.\n",
    "        geometry (numpy.ndarray): The geometry from the EAST detector.\n",
    "        confThreshold (float): Confidence threshold to filter weak detections.\n",
    "        nmsThreshold (float): Non-maxima suppression threshold.\n",
    "        rW (float): Width ratio.\n",
    "        rH (float): Height ratio.\n",
    "\n",
    "    Returns:\n",
    "        masked_image (numpy.ndarray): The resulting image with bounding boxes applied.\n",
    "    \"\"\"\n",
    "    (rects, confidences) = decode_predictions(scores, geometry, confThreshold)\n",
    "\n",
    "    # Apply non-maxima suppression to suppress weak, overlapping bounding boxes\n",
    "    boxes = non_max_suppression(np.array(rects), probs=confidences, overlapThresh=nmsThreshold)\n",
    "\n",
    "\n",
    "    mask = np.zeros(orig.shape[:2], dtype='uint8')\n",
    "\n",
    "    padding = 10  # Adjust padding as needed\n",
    "\n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # Apply padding around each bounding box and ensure the coordinates are within image bounds\n",
    "        p_startX = max(0, int(startX * rW ) - padding )\n",
    "        p_startY = max(0, int(startY * rH ) - padding)\n",
    "        p_endX = min(orig.shape[1] -1 , int( endX * rW ) + padding )\n",
    "        p_endY = min(orig.shape[0] -1 , int( endY * rH ) + padding )\n",
    "\n",
    "        # Draw white rectangles on the mask for each bounding box (with padding)\n",
    "        cv2.rectangle(mask, (p_startX, p_startY), (p_endX, p_endY), 255, -1)\n",
    "\n",
    "\n",
    "    # Apply the mask to the original image\n",
    "    masked_image = cv2.bitwise_and(orig, orig, mask=mask)\n",
    "\n",
    "    return masked_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CNN Model for applying masking around the text\n",
    "def detect_text_east(image_path):\n",
    "    \"\"\"\n",
    "    Detects text in an image using the EAST text detector model.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        result_image (numpy.ndarray): The resulting image with text bounding boxes applied.\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    orig = image.copy()\n",
    "    (Ori_H, Ori_W) = image.shape[:2]\n",
    "\n",
    "    # Set the new width and height and then determine the ratio in change\n",
    "    (newW, newH) = (320, 320)\n",
    "    rW = Ori_W / float(newW)\n",
    "    rH = Ori_H / float(newH)\n",
    "\n",
    "    # Resize the image and grab the new image dimensions\n",
    "    image = cv2.resize(image, (newW, newH))\n",
    "    (H, W) = image.shape[:2]\n",
    "\n",
    "    # Define the two output layer names for the EAST detector model\n",
    "    layerNames = [\n",
    "        \"feature_fusion/Conv_7/Sigmoid\",\n",
    "        \"feature_fusion/concat_3\"\n",
    "    ]\n",
    "\n",
    "    # Load the pre-trained EAST text detector\n",
    "    net = cv2.dnn.readNet(\"frozen_east_text_detection.pb\")\n",
    "\n",
    "    # Construct a blob from the image and then perform a forward pass\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n",
    "                                 (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    (scores, geometry) = net.forward(layerNames)\n",
    "\n",
    "    rW = Ori_W / float(newW)\n",
    "    rH = Ori_H / float(newH)\n",
    "\n",
    "    # Decode the predictions, then  apply non-maxima suppression to suppress weak, overlapping bounding boxes\n",
    "    result_image = post_process(orig.copy(), scores, geometry, confThreshold=0.5, nmsThreshold=0.4, rW=rW, rH=rH)\n",
    "    # Return the original image (for now)\n",
    "    return result_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE NAME:  sample1\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1138: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"frozen_east_text_detection.pb\" in function 'cv::dnn::ReadProtoFromBinaryFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIMAGE NAME: \u001b[39m\u001b[38;5;124m'\u001b[39m, image_name)\n\u001b[0;32m      9\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 11\u001b[0m result_image \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_text_east\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#display_image(result_image)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/images/output/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_stage_1.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[27], line 33\u001b[0m, in \u001b[0;36mdetect_text_east\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     27\u001b[0m layerNames \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_fusion/Conv_7/Sigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_fusion/concat_3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m ]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Load the pre-trained EAST text detector\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadNet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrozen_east_text_detection.pb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Construct a blob from the image and then perform a forward pass\u001b[39;00m\n\u001b[0;32m     36\u001b[0m blob \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mblobFromImage(image, \u001b[38;5;241m1.0\u001b[39m, (W, H),\n\u001b[0;32m     37\u001b[0m                              (\u001b[38;5;241m123.68\u001b[39m, \u001b[38;5;241m116.78\u001b[39m, \u001b[38;5;241m103.94\u001b[39m), swapRB\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1138: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"frozen_east_text_detection.pb\" in function 'cv::dnn::ReadProtoFromBinaryFile'\n"
     ]
    }
   ],
   "source": [
    "for image_file_name in os.listdir(folder_path):\n",
    "    image_name = image_file_name.split('.')[0]\n",
    "    image_path = folder_path + image_file_name\n",
    "\n",
    "    if image_name is None or image_name == '':\n",
    "      continue\n",
    "\n",
    "    print(f'IMAGE NAME: ', image_name)\n",
    "    start = time.time()\n",
    "\n",
    "    result_image = detect_text_east(image_path)\n",
    "    #display_image(result_image)\n",
    "    output_path = f'/images/output/{image_name}_stage_1.jpg'\n",
    "    status = cv2.imwrite(output_path, result_image)\n",
    "\n",
    "    ## SUPER RESOLUTION\n",
    "    IMAGE_PATH = f'/images/output/{image_name}_stage_1.jpg'\n",
    "    hr_image = preprocess_image(IMAGE_PATH)\n",
    "    #plot_image(tf.squeeze(hr_image), title=\"Original Image\")\n",
    "\n",
    "    fake_image = model(hr_image)\n",
    "    fake_image = tf.squeeze(fake_image)\n",
    "\n",
    "    #plot_image(tf.squeeze(fake_image), title=\"Super Resolution\")\n",
    "    save_image(tf.squeeze(fake_image), filename=f\"/images/output/{image_name}_stage_2.jpg\")\n",
    "\n",
    "    ##OCR\n",
    "\n",
    "    masked_image_path = f'/images/output/{image_name}_stage_2.jpg'\n",
    "    #masked_image_path = '/content/test_images/00bedd5c2fbf2dff.jpg'\n",
    "\n",
    "    # Read the image using OpenCV (PyTesseract also works with PIL images)\n",
    "    masked_image = cv2.imread(masked_image_path)\n",
    "\n",
    "    # Convert the image to RGB (PyTesseract expects images in RGB format)\n",
    "    masked_image_rgb = cv2.cvtColor(masked_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Use PyTesseract to extract text\n",
    "    extracted_text = pytesseract.image_to_string(masked_image_rgb)\n",
    "\n",
    "    words = extracted_text.split()\n",
    "    words = [re.sub(r'[^\\w\\s]', '', word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE NAME:  ..\\images\\test_images\\sample1.jpg\n",
      "Time Taken: 0.910790\n",
      "==================================\n",
      "Extracted Text:\n",
      "['CENTER', 'FOR', 'THE', 'i', 'hisricdomio', 'Maliue', 'January', '629', 'Fri', '', 'Saturdays', 'at', '8', 'Sander', 'Matinees', 'at', '2', 'pas', '_', 'Tickets', '7238698', '', 'wwwhenegarorg', 'Mara', 'Deremces', 'ee']\n",
      "===================================\n",
      "IMAGE NAME:  ..\\images\\test_images\\test1.png\n",
      "Time Taken: 1.674362\n",
      "==================================\n",
      "Extracted Text:\n",
      "['iasn', 'Silioss', '71', 'De', '', 'ear']\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "for image_file_name in os.listdir(folder_path):\n",
    "    start = time.time()\n",
    "\n",
    "    image_name = image_file_name.split('.')[0]\n",
    "    image_path = folder_path + image_file_name\n",
    "\n",
    "    if image_name is None or image_name == '':\n",
    "      continue\n",
    "    print(f'IMAGE NAME: ', image_path)\n",
    "    masked_image_path = image_path\n",
    "    #masked_image_path = '/content/test_images/00bedd5c2fbf2dff.jpg'\n",
    "\n",
    "    # Read the image using OpenCV (PyTesseract also works with PIL images)\n",
    "    masked_image = cv2.imread(masked_image_path)\n",
    "\n",
    "    # Convert the image to RGB (PyTesseract expects images in RGB format)\n",
    "    masked_image_rgb = cv2.cvtColor(masked_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Use PyTesseract to extract text\n",
    "    extracted_text = pytesseract.image_to_string(masked_image_rgb)\n",
    "\n",
    "    words = extracted_text.split()\n",
    "    words = [re.sub(r'[^\\w\\s]', '', word) for word in words]\n",
    "\n",
    "    print(\"Time Taken: %f\" % (time.time() - start))\n",
    "\n",
    "    print('==================================')\n",
    "    print(\"Extracted Text:\")\n",
    "    print(words)\n",
    "    print('===================================')\n",
    "\n",
    "    tesseract_dict[image_name] = words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
